# FashionMNIST Classification with Custom PyTorch Layer

A PyTorch implementation demonstrating a custom `LearnedAffine` layer and clean training loop for FashionMNIST classification.

## ğŸ¯ Project Overview

This project fulfills the requirements of a PyTorch assignment focusing on:
- Custom layer implementation (`LearnedAffine`)
- Proper training and evaluation loops
- Modern optimization techniques (AdamW + OneCycleLR)
- Reproducible results with detailed documentation

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ layers.py          # Custom LearnedAffine layer + sanity tests
â”œâ”€â”€ train.py           # Training script with eval loop
â”œâ”€â”€ metrics.md         # Training observations and analysis (150-250 words)
â”œâ”€â”€ repro.md          # Reproducibility guide with exact commands
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md         # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Test Custom Layer

```bash
python layers.py
```

This runs sanity checks on the `LearnedAffine` layer, verifying:
- âœ“ Shape preservation
- âœ“ Parameter count (2 Ã— num_features)
- âœ“ Gradient flow
- âœ“ Initial values (scale=1, shift=0)

### 3. Train Model

```bash
python train.py
```

Trains a CNN on FashionMNIST for 3 epochs with:
- **Loss**: CrossEntropyLoss
- **Optimizer**: AdamW (lr=0.001)
- **Scheduler**: OneCycleLR (max_lr=0.01)
- **Seed**: 42 (for reproducibility)

Expected output: ~88-90% test accuracy after 3 epochs.

## ğŸ§  Model Architecture

```
FashionMNISTNet(
  Conv2d(1 â†’ 32) â†’ ReLU â†’ MaxPool
  Conv2d(32 â†’ 64) â†’ ReLU â†’ MaxPool â†’ Dropout2d(0.25)
  Linear(3136 â†’ 128) â†’ ReLU
  LearnedAffine(128)     â† Custom Layer
  Dropout(0.5)
  Linear(128 â†’ 10)
)
```

**Total Parameters**: ~1.2M

## ğŸ”¬ Custom LearnedAffine Layer

The `LearnedAffine` layer applies a learnable affine transformation:

```
y = scale âŠ™ x + shift
```

Where:
- `scale`: Learnable parameter (initialized to 1)
- `shift`: Learnable parameter (initialized to 0)
- âŠ™: Element-wise multiplication

This adds flexibility between the fully connected layers, allowing the network to learn adaptive normalization.

## ğŸ“Š Training Details

### Configuration
- **Dataset**: FashionMNIST (60k train, 10k test)
- **Epochs**: 3
- **Batch Size**: 64
- **Optimizer**: AdamW (weight_decay=0.01 by default)
- **Scheduler**: OneCycleLR with max_lr=0.01
- **Seed**: 42

### Evaluation Protocol
- Model set to `eval()` mode during testing
- Gradients disabled with `torch.no_grad()`
- Metrics: Cross-entropy loss + accuracy

## ğŸ“ˆ Results

See [metrics.md](metrics.md) for detailed analysis of:
- What was tried
- What worked well
- Potential improvements

## ğŸ”„ Reproducibility

All experiments use `torch.manual_seed(42)` for reproducibility. See [repro.md](repro.md) for:
- Exact commands to reproduce results
- Seed configuration details
- Environment setup instructions
- Troubleshooting tips

## ğŸ“š Key Learning Points

1. **Custom Layer**: Implemented `nn.Module` with learnable parameters
2. **Training Loop**: Proper use of `model.train()` and optimizer steps
3. **Evaluation**: Correct use of `model.eval()` and `torch.no_grad()`
4. **Optimization**: AdamW optimizer with OneCycleLR scheduling
5. **Reproducibility**: Seed management and deterministic operations

## ğŸ“ Assignment Rubric Coverage

- âœ… **Custom layer correct + sanity test included** (3 pts)
  - `LearnedAffine` in `layers.py` with comprehensive tests
  
- âœ… **Training loop correct + eval uses eval() and no_grad()** (3 pts)
  - Proper training loop in `train.py`
  - Evaluation function with `model.eval()` and `torch.no_grad()`
  
- âœ… **Optimizer + scheduler used correctly and explained** (3 pts)
  - AdamW optimizer configured
  - OneCycleLR scheduler with per-batch updates
  - Explanations in `metrics.md`
  
- âœ… **Repro clarity (seed + commands + readable notes)** (3 pts)
  - `torch.manual_seed(42)` documented
  - Complete command instructions in `repro.md`
  - Clear documentation throughout

## ğŸ› ï¸ Requirements

- Python 3.8+
- PyTorch 2.0+
- torchvision 0.15+

## ğŸ“ License

This project is for educational purposes as part of a course assignment.

## ğŸ‘¤ Author

Created as part of a PyTorch deep learning assignment.

---

**Note**: For submission, upload this entire repository to GitHub and submit the repository link.
